{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b8ffe6-9e99-4ada-a382-9280c9df1ebc",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Breast cancer remains one of the leading causes of cancer-related deaths among women globally. Despite advances in medical technology, early diagnosis and accurate classification of breast cancer types remain critical challenges in clinical practice. Misclassification or delayed diagnosis can lead to ineffective treatment plans, increased patient suffering, and higher mortality rates.\n",
    "\n",
    "Traditional diagnostic methods, such as mammography and biopsy analysis, are often time-consuming, costly, and sometimes prone to human error due to subjective interpretation by medical professionals. Therefore, there is a pressing need for reliable, automated systems that can assist healthcare providers by accurately identifying the type of breast cancer from patient data.\n",
    "\n",
    "The objective of this project is to develop a machine learning-based classification system that can analyze clinical features extracted from breast tissue samples to distinguish between different types of breast cancer (e.g., benign, malignant, and other subtypes). This system aims to:\n",
    "\n",
    "- Improve diagnostic accuracy and consistency.\n",
    "\n",
    "- Reduce the time and cost associated with manual diagnosis.\n",
    "\n",
    "- Provide a decision-support tool that complements radiologists and pathologists.\n",
    "\n",
    "- Facilitate early detection to enhance patient prognosis and survival rates.\n",
    "\n",
    "By leveraging a well-structured dataset with relevant features and applying advanced AI techniques, this project seeks to contribute to more precise breast cancer diagnosis and ultimately improve patient\n",
    "\n",
    "# Dataset Description\n",
    "\n",
    "The dataset used for this project is the Breast Cancer Wisconsin (Diagnostic) Dataset (or whichever specific breast cancer dataset you choose). It is sourced from UCI Machine Learning Repository.The dataset employed in this study is the Breast Cancer Wisconsin (Diagnostic) Dataset, a widely recognized benchmark dataset in the domain of medical diagnostics and machine learning. This dataset comprises clinical attributes extracted from digitized images of fine needle aspirate (FNA) biopsies, representing various morphological characteristics of breast tissue cells.\n",
    "\n",
    "- The dataset contains multiple features (at least 4) derived from digitized images of fine needle aspirate (FNA) of breast masses.\n",
    "\n",
    "- It includes a label column representing the diagnosis classes, typically categorized into at least three classes such as benign, malignant, and    possibly atypical or other subtypes.\n",
    "\n",
    "- The dataset consists of more than 100 samples, ensuring adequate data for training and validation.\n",
    "\n",
    "# Dataset Characteristics\n",
    "- Number of samples: ~569 \n",
    "\n",
    "- Number of features: 7 \n",
    "\n",
    "- Classes: 3\n",
    "\n",
    "# AI Techniques Planned\n",
    "- Data Preprocessing: Handling missing data, normalization, and feature selection.\n",
    "  \n",
    "- Evaluation Metrics: Accuracy, Precision, Recall, F1-Score, and Confusion Matrix to assess model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb69500-8704-453c-b716-3f6068688123",
   "metadata": {},
   "source": [
    "# Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf600e7-e355-40a8-9e92-1027cc909db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbb750-a262-4f9e-b500-1495b8910f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open('data.csv', 'rb') as f:\n",
    "    rawdata = f.read()\n",
    "\n",
    "result = chardet.detect(rawdata)\n",
    "encoding = result['encoding']\n",
    "print(f\"Detected encoding: {encoding}\")\n",
    "\n",
    "with open('data.csv', encoding=encoding) as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f020e470-7e8d-4aa2-ac81-fab3ff700409",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mparsers.pyx:1120\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1272\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1285\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1535\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1066\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1127\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1272\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1285\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1535\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', encoding='utf-8', on_bad_lines='skip')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302821b-c5f8-43bf-a4eb-368cafc02024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4007468-4985-46b2-aa10-fad26af7d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f821c06-5f55-4aaa-bdc2-f0e49c53fe89",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc778c4b-5561-4752-9cd1-aab4a580d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:9, 'diagnosis'] = 'U'\n",
    "df.loc[df['area_mean'] < 300, 'diagnosis'] = 'U'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f153f-7300-4976-be71-cc8876ad35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Choose 5% of rows randomly\n",
    "num_rows = int(0.05 * len(df))\n",
    "random_indices = np.random.choice(df.index, size=num_rows, replace=False)\n",
    "\n",
    "# Assign new class 'U' to those rows\n",
    "df.loc[random_indices, 'diagnosis'] = 'U'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda78d8b-2c98-4c8f-ae86-9fec73b48b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"area_worst\",\n",
    "    \"concave points_worst\",\n",
    "    \"concave points_mean\",\n",
    "    \"radius_worst\",\n",
    "    \"perimeter_worst\",\n",
    "    \"perimeter_mean\",\n",
    "    \"concavity_mean\",\n",
    "    \"area_mean\",\n",
    "    \"diagnosis\"\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883b56c-7cd9-4824-b14a-fcaf8b130a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45a6fc-6a72-40bb-abe3-6df777c8d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1d317-1a2d-403d-9d3a-a1cc16951116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499418cf-e689-43a5-8ae5-ace1dc350f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73a6c5-b7ba-4fff-bfaa-f128cac4f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_all = df[df.duplicated()]\n",
    "print(duplicates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e32db9-cbae-4eb4-973d-53f4e676ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate data\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cb450-93d1-41c1-94e8-2ebe9f8dae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes  Nan value .\n",
    "df.dropna(inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3612ed0-1d31-4807-b64a-e69d89c9b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Nan Value in the dataframe\n",
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e0f5a-9f61-4431-91f8-943b1ffb376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the nan value\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f71f6b-2255-48ff-b8e5-349c31291f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633ffdf-acf2-4b1d-952e-57fa55c0e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area_worst'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c59f63-6691-47a8-b0ee-12271efc07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concave points_worst'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65112f26-068c-416f-b9e1-70d5d449189e",
   "metadata": {},
   "outputs": [],
   "source": [
    " df['concave points_mean'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba5f7e-be61-4079-bfc6-ead2d87afb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    " df['radius_worst'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d45037-266d-45d7-bed3-66861efaa5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    " df['perimeter_worst'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f7968-c2b0-4586-9282-1ab06669162e",
   "metadata": {},
   "outputs": [],
   "source": [
    " df['perimeter_mean'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955981b-29da-4745-8a61-4cf906fd8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concavity_mean'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf7d46-c383-4b5a-889b-091df758e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area_mean'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96ff5d-2085-48f5-8934-db5dfd2aae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bbc4d-b9e7-4d77-8756-825123de5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['diagnosis'].unique())\n",
    "print(df['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1031b7-e6bf-4f48-bb14-4f5d408984c9",
   "metadata": {},
   "source": [
    "# Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e7e9a-a14f-4bf9-914c-0e6177396b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'diagnosis' column: M = 1, B = 0\n",
    "#df[\"diagnosis\"] = df[\"diagnosis\"].map({'M': 1, 'B': 0})\n",
    "df['diagnosis_encoded'] = df['diagnosis'].map({'M': 1, 'B': 0, 'U': 2})\n",
    "\n",
    "\n",
    "# Display the updated column to confirm\n",
    "df[\"diagnosis\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e1f0d-778e-48fc-98e5-f4d2b47814c0",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41baf342-8bfd-40af-b1f3-427aa1d55827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cced7-88b1-4cf7-aca0-c3244f510369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(1, 3, 1)  \n",
    "sns.histplot(df['area_worst'], kde=True, color='blue')  \n",
    "\n",
    "# Second subplot\n",
    "plt.subplot(1, 3, 2)  \n",
    "sns.histplot(df['concave points_worst'], kde=True, color='green')  \n",
    "# Third subplot\n",
    "plt.subplot(1, 3, 3)  # Third column\n",
    "sns.histplot(df['radius_worst'], kde=True, color='red') \n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a8952-5d62-4364-a72f-ff21e4785494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(1, 3, 1)  \n",
    "sns.histplot(df['perimeter_worst'], kde=True, color='blue')  \n",
    "\n",
    "# Second subplot\n",
    "plt.subplot(1, 3, 2)  \n",
    "sns.histplot(df['perimeter_mean'], kde=True, color='green')  \n",
    "# Third subplot\n",
    "plt.subplot(1, 3, 3)  # Third column\n",
    "sns.histplot(df['concavity_mean'], kde=True, color='red') \n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052e9ab-1d3f-4b26-952f-e9fd1dbdf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(1, 3, 1)  \n",
    "sns.histplot(df['area_mean'], kde=True, color='blue')  \n",
    "\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398fb19-8fef-463f-b4be-cd614423228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area_worst'].skew()\n",
    "df['area_worst'].describe()\n",
    "sns.boxplot(df['area_worst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72343eeb-45ec-4f4a-9009-cda9e1310b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile25 = df['area_worst'].quantile(0.25)\n",
    "percentile75 = df['area_worst'].quantile(0.75)\n",
    "percentile75\n",
    "iqr = percentile75 - percentile25\n",
    "iqr\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"upperlimit\", upper_limit)\n",
    "print(\"lowerlimit\", lower_limit)\n",
    "df[df['area_worst']>upper_limit]\n",
    "df[df['area_worst'] < lower_limit]\n",
    "df = df[df['area_worst'] < upper_limit]\n",
    "df.shape\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 3)  \n",
    "sns.histplot(df['area_worst'], kde=True, color='blue')\n",
    "plt.subplot(2, 2, 4) \n",
    "sns.boxplot(df['area_worst'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c7429-6565-4373-9c98-6756ac577045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concave points_worst'].skew()\n",
    "df['concave points_worst'].describe()\n",
    "sns.boxplot(df['concave points_worst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee63165-4f12-4e46-9237-670c0752207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concave points_mean'].skew()\n",
    "df['concave points_mean'].describe()\n",
    "sns.boxplot(df['concave points_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537d4e2-e7e0-4297-ae7f-1d529d78cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile25 = df['concave points_mean'].quantile(0.25)\n",
    "percentile75 = df['concave points_mean'].quantile(0.75)\n",
    "percentile75\n",
    "iqr = percentile75 - percentile25\n",
    "iqr\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"upperlimit\", upper_limit)\n",
    "print(\"lowerlimit\", lower_limit)\n",
    "df[df['concave points_mean']>upper_limit]\n",
    "df[df['concave points_mean'] < lower_limit]\n",
    "df = df[df['concave points_mean'] < upper_limit]\n",
    "df.shape\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 3)  \n",
    "sns.histplot(df['concave points_mean'], kde=True, color='blue')\n",
    "plt.subplot(2, 2, 4) \n",
    "sns.boxplot(df['concave points_mean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bdf613-2436-4e2f-b456-9b91ba3cd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['radius_worst'].skew()\n",
    "df['radius_worst'].describe()\n",
    "sns.boxplot(df['radius_worst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5169b-4d78-4dab-95dc-37db7560e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile25 = df['radius_worst'].quantile(0.25)\n",
    "percentile75 = df['radius_worst'].quantile(0.75)\n",
    "percentile75\n",
    "iqr = percentile75 - percentile25\n",
    "iqr\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"upperlimit\", upper_limit)\n",
    "print(\"lowerlimit\", lower_limit)\n",
    "df[df['radius_worst']>upper_limit]\n",
    "df[df['radius_worst'] < lower_limit]\n",
    "df = df[df['radius_worst'] < upper_limit]\n",
    "df.shape\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 3)  \n",
    "sns.histplot(df['radius_worst'], kde=True, color='blue')\n",
    "plt.subplot(2, 2, 4) \n",
    "sns.boxplot(df['radius_worst'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526eb946-ff14-4dc2-b3f6-c34401480f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['perimeter_worst'].skew()\n",
    "df['perimeter_worst'].describe()\n",
    "sns.boxplot(df['perimeter_worst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e0008-0b9c-478b-9105-8e9a3a7fa1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile25 = df['perimeter_worst'].quantile(0.25)\n",
    "percentile75 = df['perimeter_worst'].quantile(0.75)\n",
    "percentile75\n",
    "iqr = percentile75 - percentile25\n",
    "iqr\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"upperlimit\", upper_limit)\n",
    "print(\"lowerlimit\", lower_limit)\n",
    "df[df['perimeter_worst']>upper_limit]\n",
    "df[df['perimeter_worst'] < lower_limit]\n",
    "df = df[df['perimeter_worst'] < upper_limit]\n",
    "df.shape\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 3)  \n",
    "sns.histplot(df['perimeter_worst'], kde=True, color='blue')\n",
    "plt.subplot(2, 2, 4) \n",
    "sns.boxplot(df['perimeter_worst'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd3630-9baf-4a8d-b731-a9792f340069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['perimeter_mean'].skew()\n",
    "df['perimeter_mean'].describe()\n",
    "sns.boxplot(df['perimeter_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d383c-1264-438e-a467-09b07f61c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile25 = df['perimeter_mean'].quantile(0.25)\n",
    "percentile75 = df['perimeter_mean'].quantile(0.75)\n",
    "percentile75\n",
    "iqr = percentile75 - percentile25\n",
    "iqr\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"upperlimit\", upper_limit)\n",
    "print(\"lowerlimit\", lower_limit)\n",
    "df[df['perimeter_mean']>upper_limit]\n",
    "df[df['perimeter_mean'] < lower_limit]\n",
    "df = df[df['perimeter_mean'] < upper_limit]\n",
    "df.shape\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 3)  \n",
    "sns.histplot(df['perimeter_mean'], kde=True, color='blue')\n",
    "plt.subplot(2, 2, 4) \n",
    "sns.boxplot(df['perimeter_mean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec6730-caee-4340-ab4a-db4793521382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concavity_mean'].skew()\n",
    "df['concavity_mean'].describe()\n",
    "sns.boxplot(df['concavity_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb07040-4958-48e1-8ab9-486a7ad11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile25 = df['concavity_mean'].quantile(0.25)\n",
    "percentile75 = df['concavity_mean'].quantile(0.75)\n",
    "percentile75\n",
    "iqr = percentile75 - percentile25\n",
    "iqr\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"upperlimit\", upper_limit)\n",
    "print(\"lowerlimit\", lower_limit)\n",
    "df[df['concavity_mean']>upper_limit]\n",
    "df[df['concavity_mean'] < lower_limit]\n",
    "df = df[df['concavity_mean'] < upper_limit]\n",
    "df.shape\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 3)  \n",
    "sns.histplot(df['concavity_mean'], kde=True, color='blue')\n",
    "plt.subplot(2, 2, 4) \n",
    "sns.boxplot(df['concavity_mean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245b343-c2c7-4a2e-908a-2d2ac9d8ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area_mean'].skew()\n",
    "df['area_mean'].describe()\n",
    "sns.boxplot(df['area_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed0aad-de4b-497e-b12a-e143bae231a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile25 = df['area_mean'].quantile(0.25)\n",
    "percentile75 = df['area_mean'].quantile(0.75)\n",
    "percentile75\n",
    "iqr = percentile75 - percentile25\n",
    "iqr\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"upperlimit\", upper_limit)\n",
    "print(\"lowerlimit\", lower_limit)\n",
    "df[df['area_mean']>upper_limit]\n",
    "df[df['area_mean'] < lower_limit]\n",
    "df = df[df['area_mean'] < upper_limit]\n",
    "df.shape\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 3)  \n",
    "sns.histplot(df['area_mean'], kde=True, color='blue')\n",
    "plt.subplot(2, 2, 4) \n",
    "sns.boxplot(df['area_mean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec7dc7-77ff-4c90-a42e-c89b4b28e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"Unique classes in original df:\", df['diagnosis'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b35bda-b5f6-4ab9-9926-cb10e741979e",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d93232-1825-4e01-9ef1-fc839ca3fb7c",
   "metadata": {},
   "source": [
    "# MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1dfb88-1e3c-4763-8ee5-99b8e6db2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144138b-c967-4c0f-8a20-d54415264074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is your original DataFrame and diagnosis is your target column\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219b3a8-4deb-42c8-b2ba-97197b4b10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Step 3: Apply MinMax scaling\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa1bcc7-ea47-435f-aae7-031c6d2ef63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Important: set original index to avoid NaNs during concat\n",
    "minmax_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "# Now concatenate with y\n",
    "final_minmax_df = pd.concat([minmax_scaled_df, y], axis=1)\n",
    "\n",
    "# Step 6: Display result\n",
    "print(final_minmax_df.head())\n",
    "\n",
    "print(final_minmax_df.isna().sum())  # Should be all 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9d9f5-41f7-4c5e-91bf-7be80307567f",
   "metadata": {},
   "source": [
    "# Standization (Z-score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867f661-0012-4279-92af-99934975f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df is your original DataFrame and diagnosis is your target column\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Fit and transform scaler on entire X\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# When creating DataFrame, preserve the original index!\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "# Now concatenate with y, indices align perfectly\n",
    "final_scaled_df = pd.concat([X_scaled_df, y], axis=1)\n",
    "# Step 6: Display result\n",
    "print(final_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030a451-fc56-4e40-85c0-37df1bb87120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_scaled_df.isna().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a837cb-8e5b-4b1b-879a-bb790fa6891f",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121e593-1cb9-4658-97b7-3ac8daad3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fe305-9b70-4959-9356-bd4db0a0719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Encode text labels into numbers (needed for `c=...`)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2fb2b-8599-4805-b33f-2759c2bb14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fee4b-f706-4d3a-a2a7-99e96425450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076718c4-64f2-47d7-b4fb-9d2371d399fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525f2eb-acaf-4c8a-be05-477f217e138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8227437-abf7-4a12-b0a9-d382581a975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5fcf6-697d-494e-998d-f7356a8fc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap='plasma')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.clim(-0.5, len(le.classes_)-0.5)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac9559-5ab3-486b-a7e7-cff79bc71ee1",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0e186-7911-4def-b733-795c8de684cf",
   "metadata": {},
   "source": [
    "#Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46ee6b-7857-4077-98db-ce95f652b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='diagnosis', y='concavity_mean', data=df, palette='pastel')\n",
    "plt.title('Concavity Mean by Diagnosis')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('concavity_mean')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b774ca9-8f8c-4693-9e2c-76e6cc0870c7",
   "metadata": {},
   "source": [
    "#Violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14fcef-731a-417f-a54b-e316148e64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=df['concave points_mean'], color='lightgreen')\n",
    "plt.title('Violin Plot of concave points_mean')\n",
    "plt.xlabel('concave points_mean')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b98707-1541-4f65-9f7a-cb867a7b7f38",
   "metadata": {},
   "source": [
    "#Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f2909-ec1a-4c5a-8bd2-695298ec82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='radius_worst', hue='diagnosis', kde=True, palette='Set2')\n",
    "plt.title('Radius Worst by Diagnosis')\n",
    "plt.xlabel('radius_worst')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825fcef1-5a37-45c3-b6e3-a69619d7a75a",
   "metadata": {},
   "source": [
    "#Stripplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500885e5-00b4-4375-aeb2-35f97b53dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x='diagnosis', y='perimeter_worst', data=df, jitter=True, palette='coolwarm')\n",
    "plt.title('Perimeter Worst by Diagnosis')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('perimeter_worst')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0ccaf-ecb2-43fe-bb30-d515398c45e6",
   "metadata": {},
   "source": [
    "#Swarmplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfdff3-54c7-4504-9367-56c846c59293",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x='diagnosis', y='perimeter_mean', data=df, palette='Set1')\n",
    "plt.title('Perimeter Mean by Diagnosis')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('perimeter_mean')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30682ae-d593-4a77-a108-973eeac7eefb",
   "metadata": {},
   "source": [
    "#KDE Plot (Kernel Density Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72bc98-1a10-4e63-b417-cae97fff44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x='area_mean', hue='diagnosis', fill=True, palette='husl')\n",
    "plt.title('Density Plot of area_mean by Diagnosis')\n",
    "plt.xlabel('area_mean')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194170b9-b8eb-4e2c-9966-b54ce78e36a4",
   "metadata": {},
   "source": [
    "# Implementation & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d175a-0446-469e-8ab3-9d746cefbd0e",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62c5b5-37d9-4460-a0c0-8e849b0529d1",
   "metadata": {},
   "source": [
    "# MLP Claasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4320c54-0410-4f12-bb02-94df6bea1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38805a3-229c-4c47-b80b-a4f42f94a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split features and target\n",
    "X = final_scaled_df.drop('diagnosis', axis=1)\n",
    "y = final_scaled_df['diagnosis']\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Define the MLP model with regularization to reduce overfitting\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(30, 15, 20),\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "# Step 4: Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate performance\n",
    "accuracy1 = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - MLP Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334192a-860a-47a2-af50-c57a02a7eb4b",
   "metadata": {},
   "source": [
    "# Hyperparametr Tuning (GridSearchCV )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7144db-ff76-4edd-bb40-5243592cd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208093f3-47e9-4dd1-97c7-df1065a629b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define the model\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(30, 15, 20), (50, 30, 20), (30, 30, 30)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit to training data (X_train, y_train from your split)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Use best estimator to predict test set\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Accuracy with best MLP:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145fe85-549f-46d8-a950-9ed28f32ef31",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7946e1-2e5e-4819-b92b-8be6c0b7925c",
   "metadata": {},
   "source": [
    "# Ensembel model(Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1355705-435e-4e8c-9389-2eb13ea67ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fc17a-cbef-4f25-93dd-39f8656089eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging ensemble with your MLP as base estimator\n",
    "bagging_mlp = BaggingClassifier(\n",
    "    estimator=mlp,\n",
    "    n_estimators=10,         # 10 MLP models in the ensemble\n",
    "    max_samples=0.8,         # Each base model trained on 80% random subset\n",
    "    max_features=1.0,        # Use all features\n",
    "    bootstrap=True,          # Sampling with replacement\n",
    "    random_state=42,\n",
    "    n_jobs=-1                # Use all CPU cores for parallelism\n",
    ")\n",
    "\n",
    "# Train on your scaled training data\n",
    "bagging_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on your scaled test data\n",
    "y_pred = bagging_mlp.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Bagging MLP Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Bagging MLP Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548bd4f-209e-4bfe-a79c-565953753fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris  # Replace with your dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f5748-82f5-4645-ae0c-18187ba327e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 ensemble models:\n",
    "\n",
    "# 1. Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# 2. Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "# 3. Bagging (using Decision Trees as base estimators)\n",
    "bagging_model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bag = bagging_model.predict(X_test)\n",
    "bag_accuracy = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "# 4. Voting Classifier (ensemble of MLP, Random Forest, and Gradient Boosting)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('mlp', mlp_model),\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model)\n",
    "    ],\n",
    "    voting='hard'  # majority voting\n",
    ")\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"MLP Accuracy (Task 1): {mlp_accuracy:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
    "print(f\"Bagging Accuracy: {bag_accuracy:.4f}\")\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ec578-2b58-4016-a0b7-9b295700c544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc87479-ddf1-425b-b684-92cbfb8986d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498bd11-2b7a-405c-8589-b69fdca80179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a405a11-cd8f-4b2b-9dbf-2aad832e940f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abf7c4-1d6c-4bfa-a76c-d371717607b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b55de34a-a5d3-458b-a7f1-fd276b761f5e",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed895e95-821d-4c54-b702-464a60393f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison\n",
    "print(f\"Single MLP Accuracy: {accuracy1:.4f}\")\n",
    "print(f\"Bagging Ensemble MLP Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc518d-fda1-477c-aebd-7336b89d63fe",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445660d-5ed8-4e48-877f-ef523faf6ebc",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a554b-0862-420a-8252-626a73cac1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow scikit-learn pandas matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911dcba7-592a-4dc3-8d94-f84c41fa880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054863ed-65fc-4d74-a832-e9d8fc361b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data\n",
    "X = final_scaled_df.drop('diagnosis', axis=1).values\n",
    "y = LabelEncoder().fit_transform(final_scaled_df['diagnosis'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape for Conv1D: (samples, features, 1)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', padding='same', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    Flatten(),\n",
    "    \n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Predict\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Evaluate\n",
    "print(\"CNN Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - CNN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8561b3-fe64-4ecf-99d4-bfc5e8089264",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170f9ea-5ec7-4f12-8ede-adefff801c12",
   "metadata": {},
   "source": [
    "#  clustering(Gaussian Mixture Model (GMM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9ab85-3dab-476d-9f36-bf8d0fbdd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21cc81-afba-4bd3-9b39-567d49e9589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5acd5-b701-407c-8a11-ec3a529782c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate Clustering\n",
    "ari = adjusted_rand_score(y_encoded, gmm_labels)\n",
    "sil_score = silhouette_score(X, gmm_labels)\n",
    "\n",
    "print(f\"\\n Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\" Silhouette Score: {sil_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3f552-00d0-4648-9f33-51314f86c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=gmm_labels, palette='Set1')\n",
    "plt.title(\"GMM Clustering Visualization\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title=\"GMM Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2116074-a752-4b8d-9c75-92d6ed83129f",
   "metadata": {},
   "source": [
    "## Clustering( Agglomerative Clustering )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61c038-5ad8-454b-94fe-3e0dd45ac44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857566b-d0aa-4c33-96e8-921d4df20d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Plot dendrogram to choose number of clusters\n",
    "linked = linkage(X_scaled, method='ward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9347894-97d3-4ea9-8fc3-e2aa3e4bc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=False)\n",
    "plt.title('Dendrogram - Hierarchical Clustering')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ea5e0-7288-436a-9e52-7a835812636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fit Agglomerative Clustering\n",
    "# Based on dendrogram, assume 2 clusters (Malignant, Benign)\n",
    "agg_cluster = AgglomerativeClustering(n_clusters=2, metric='euclidean', linkage='ward')\n",
    "cluster_labels = agg_cluster.fit_predict(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e95b03-5df7-4b2d-9cfc-6a2b7433fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate clustering performance\n",
    "# Map clusters to true labels (optional flip if needed)\n",
    "# Use adjusted rand index for label-agnostic comparison\n",
    "ari = adjusted_rand_score(y, cluster_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "# Silhouette Score (measures cohesion and separation)\n",
    "sil_score = silhouette_score(X_scaled, cluster_labels)\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f5f6e-8f48-46e8-9ef3-e3dd87a9ab31",
   "metadata": {},
   "source": [
    "# compaire the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca62f8-bc93-4b51-8126-5be8f19318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Agglomerative Clustering\")\n",
    "ari = adjusted_rand_score(y, cluster_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "# Silhouette Score (measures cohesion and separation)\n",
    "sil_score = silhouette_score(X_scaled, cluster_labels)\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "print(\"Gaussian Mixture\")\n",
    "ari = adjusted_rand_score(y_encoded, gmm_labels)\n",
    "sil_score = silhouette_score(X, gmm_labels)\n",
    "print(f\"\\n Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\" Silhouette Score: {sil_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
